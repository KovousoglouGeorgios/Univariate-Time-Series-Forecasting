import optuna
import numpy as np
import pandas as pd
import time
from neuralforecast import NeuralForecast
from neuralforecast.models import DeepAR
from neuralforecast.losses.pytorch import DistributionLoss
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import matplotlib.pyplot as plt
import warnings

class DeepARModel:
    def __init__(self, train_real, test_real):
        # Initialize the class with training and test data
        self.train_deepar = self.deepar_form_df(train_real)
        self.test_deepar = self.deepar_form_df(test_real)
        self.study = None
        self.best_params = None
        self.best_model = None
        self.forecast = None

    def deepar_form_df(self, df):
        # Format the input DataFrame for DeepAR
        df_deepar = df.copy()
        df_deepar = df_deepar.reset_index()
        df_deepar = df_deepar.rename(columns={'DATE OCC': 'ds', 'Occurrence Count': 'y'})
        df_deepar["unique_id"] = 0
        df_deepar['ylag14'] = df_deepar['y'].shift(14)
        df_deepar.loc[0:13, "ylag14"] = df_deepar.loc[0:13, "y"]
        df_deepar = df_deepar.reset_index().rename(columns={"index": "trend"})
        cols = df_deepar.columns
        cols = ['ds', 'unique_id','y', 'trend', 'ylag14']
        df_deepar = df_deepar[cols]
        df_deepar.ds = pd.to_datetime(df_deepar.ds)
        return pd.DataFrame(df_deepar)

    def objective(self, trial):
        # Define hyperparameters to optimize using Optuna
        input_size = trial.suggest_int('input_size', 1, 360)
        lstm_n_layers = trial.suggest_int('lstm_n_layers', 2, 8)
        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)
        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512, 1028])
        lstm_hidden_size = trial.suggest_int('lstm_hidden_size', 30, 200)
        lstm_dropout = trial.suggest_loguniform('lstm_dropout', 1e-5, 0.2)
        scaler_type = trial.suggest_categorical('scaler_type', ['standard', 'robust'])
    
        model = DeepAR(
            h=14,
            input_size=input_size,
            lstm_n_layers=lstm_n_layers,
            loss=DistributionLoss(distribution='Poisson', level=[80, 90], return_params=False),
            learning_rate=learning_rate,
            scaler_type=scaler_type,
            enable_progress_bar=True,
            batch_size=batch_size,
            lstm_hidden_size=lstm_hidden_size,
            lstm_dropout=lstm_dropout,      
            max_steps=100,
            val_check_steps=10,
            early_stop_patience_steps=-1
        )
            
        mape_values = []

        n_splits = 3
        split_size = len(self.train_deepar) // n_splits

        for i in range(n_splits):
            start_idx = i * split_size
            end_idx = start_idx + split_size

            train_split = self.train_deepar[start_idx:end_idx]
            val_split = self.train_deepar[end_idx:end_idx + 14]

            if len(train_split) == 0 or len(val_split) == 0:
                print(f"Trial {i} has Empty dataset")
                continue

            nf = NeuralForecast(models=[model], freq='D')
            nf.fit(train_split)
        
            forecast = nf.predict().reset_index()
        
            mape_value = mean_absolute_percentage_error(val_split.y, forecast["DeepAR"])
            mape_values.append(mape_value)

        mean_mape = sum(mape_values) / (n_splits - len(mape_values))
            
        return mape_value

    def optimize_hyperparameters(self, n_trials=100):
        # Perform hyperparameter optimization using Optuna
        self.study = optuna.create_study(direction='minimize')
        self.study.optimize(self.objective, n_trials=n_trials)
        self.best_params = self.study.best_params

    def train_best_model(self):
        # Train the best DeepAR model with the optimized hyperparameters
        best_model_params = {
            **self.best_params,
            'h': 14,
            'loss': DistributionLoss(distribution='Poisson', level=[80, 90], return_params=False),
            'enable_progress_bar': True,
            'max_steps': 100,
            'val_check_steps': 10,
            'early_stop_patience_steps': -1
        }
        self.best_model = NeuralForecast(
            models=[DeepAR(**best_model_params)],
            freq='D'
        )
        self.best_model.fit(self.train_deepar, val_size=14)

    def make_forecast(self):
        # Make a forecast using the best trained model
        self.forecast = self.best_model.predict(self.test_deepar)

    def evaluate_forecast(self):
        # Evaluate the forecasted values against the test data
        rmse_value = np.sqrt(mean_squared_error(self.test_deepar.y, self.forecast.DeepAR))
        mape_value = mean_absolute_percentage_error(self.test_deepar.y, self.forecast.DeepAR)

        def smape(actual, forecast):
            return 1 / len(actual) * np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)) * 100)

        smape_value = smape(np.array(self.test_deepar.y), np.array(self.forecast.DeepAR))

        print("The best model metrics are:")
        print(f" RMSE: {rmse_value}")
        print(f" MAPE: {mape_value}")
        print(f" SMAPE: {smape_value}")

    def plot_forecast(self):
        # Plot the forecasted values against the true test data
        plt.figure(figsize=(12, 6))
        time_index = self.test_deepar.ds
        plt.plot(time_index, self.test_deepar.y, label='True Test Data', color='blue', linewidth=2)
        plt.plot(time_index, self.forecast.DeepAR, label='Predicted Values', color='red', linestyle='dashed', linewidth=2)
        plt.xlabel('Time')
        plt.ylabel('Value')
        plt.title('Time Series Forecasting - Predicted vs. True Test Data')
        plt.legend()
        plt.grid(True)
        plt.show()
